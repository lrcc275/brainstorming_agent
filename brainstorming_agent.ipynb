{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c267c1",
   "metadata": {},
   "source": [
    "没有rag的版本\n",
    "直接通过与LLM的交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79cc9731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "===================== 第1轮对话 ======================\n",
      "\n",
      "【神经科学专家的回答】\n",
      "作为神经科学专家，设计基于人工视网膜的视觉修复系统需要整合多学科知识（神经工程、材料科学、计算机视觉等）。以下是分步骤的实现框架：\n",
      "\n",
      "---\n",
      "\n",
      "### **1. 系统核心目标**\n",
      "- **适用人群**：视网膜色素变性（RP）、年龄相关性黄斑变性（AMD）等光感受器退化患者。\n",
      "- **功能替代**：通过人工装置替代受损的光感受器，将光信号转化为电信号，刺激残存视网膜神经元（如双极细胞或神经节细胞）。\n",
      "\n",
      "---\n",
      "\n",
      "### **2. 关键组件与技术实现**\n",
      "\n",
      "#### **（1）图像采集与处理**\n",
      "- **微型摄像头**：植入眼镜或眼内，实时捕捉场景。\n",
      "- **预处理算法**：\n",
      "  - 动态范围压缩（适应明暗变化）\n",
      "  - 边缘增强（突出关键轮廓）\n",
      "  - 降噪（减少信息冗余）\n",
      "  - *示例*：仿生视网膜的“中心-外周抑制”处理（模拟自然视网膜的神经节细胞感受野）。\n",
      "\n",
      "#### **（2）人工视网膜植入体**\n",
      "- **电极阵列类型**：\n",
      "  - **表面型**（如Argus II）：刺激神经节细胞，需高电流但易植入。\n",
      "  - **穿透型**（如犹他电极阵列）：直接刺激双极细胞，分辨率更高但手术风险大。\n",
      "- **材料选择**：\n",
      "  - 生物兼容性材料（如聚二甲硅氧烷PDMS、氮化钛电极）。\n",
      "  - 柔性基底（减少组织损伤）。\n",
      "\n",
      "#### **（3）信号编码策略**\n",
      "- **脉冲编码模式**：\n",
      "  - 亮度→脉冲频率调制（如Phosphene感知）。\n",
      "  - 空间信息→多电极时空激活模式（需匹配视网膜拓扑图）。\n",
      "- **个性化校准**：患者通过反馈训练系统优化信号-感知映射。\n",
      "\n",
      "#### **（4）无线供能与数据传输**\n",
      "- **射频耦合**（如Argus II外部线圈供电）。\n",
      "- **近红外光供能**（实验阶段，可实现全植入式设计）。\n",
      "\n",
      "---\n",
      "\n",
      "### **3. 临床挑战与解决方案**\n",
      "- **分辨率限制**：\n",
      "  - 现状：Argus II仅60电极，提供粗略光点（Phosphene）。\n",
      "  - 改进方向：高密度阵列（如波士顿视网膜植入体的256电极）+ 动态电流聚焦技术。\n",
      "- **生物相容性**：\n",
      "  - 涂层技术（如PEDOT:PSS降低阻抗）。\n",
      "  - 抗纤维化药物缓释（如地塞米松电极涂层）。\n",
      "- **长期稳定性**：\n",
      "  - 自密封封装（防体液渗透）。\n",
      "  - 自适应阻抗监测（补偿电极老化）。\n",
      "\n",
      "---\n",
      "\n",
      "### **4. 前沿研究方向**\n",
      "- **光遗传学辅助**：基因改造残存细胞表达光敏蛋白（如ChR2），结合光学刺激（如IRBE）。\n",
      "- **人工突触器件**：忆阻器阵列模拟视网膜突触可塑性，实现类神经处理。\n",
      "- **闭环反馈**：集成fMRI或EEG实时调整刺激参数。\n",
      "\n",
      "---\n",
      "\n",
      "### **5. 伦理与未来展望**\n",
      "- **非人类感知风险**：患者可能需学习“解码”非自然视觉信号。\n",
      "- **成本与普及**：当前系统约15万美元，需推动材料与制造革新。\n",
      "- **终极目标**：与视觉皮层接口结合，实现全通路修复。\n",
      "\n",
      "---\n",
      "\n",
      "通过跨学科协作和迭代优化，人工视网膜系统有望从当前的低分辨率“视觉假体”发展为接近自然视觉的神经修复技术。\n",
      "--------------------------------------------------\n",
      "\n",
      "【深度学习专家的回答】\n",
      "# 基于人工视网膜的视觉修复系统设计\n",
      "\n",
      "作为深度学习专家，我将为您设计一个基于人工视网膜的视觉修复系统。这种系统旨在帮助视觉受损患者恢复部分视觉功能。\n",
      "\n",
      "## 系统架构概述\n",
      "\n",
      "该系统由以下几个核心组件构成：\n",
      "\n",
      "1. **人工视网膜传感器阵列**：模拟视网膜感光细胞的微型光电传感器\n",
      "2. **神经信号编码器**：将视觉信息转换为神经脉冲模式\n",
      "3. **深度学习处理模块**：增强和优化视觉信号\n",
      "4. **神经刺激接口**：将处理后的信号传递给视神经或视觉皮层\n",
      "\n",
      "## 详细实现方案\n",
      "\n",
      "### 1. 人工视网膜传感器设计\n",
      "\n",
      "- 使用高密度CMOS或有机光电传感器阵列\n",
      "- 模拟视网膜的中央凹(fovea)和周边区域的不同分辨率\n",
      "- 动态范围适应：10^6以上，模仿人眼适应能力\n",
      "\n",
      "### 2. 神经信号编码器\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from spikingjelly.activation_based import neuron, encoding\n",
      "\n",
      "class RetinalEncoder:\n",
      "    def __init__(self, temporal_stride=10, out_features=128):\n",
      "        self.temporal_encoder = encoding.PoissonEncoder()  # 泊松编码模拟神经发放\n",
      "        self.spatial_encoder = encoding.LatencyEncoder()   # 延迟编码模拟视网膜神经节细胞\n",
      "        \n",
      "    def encode(self, visual_input):\n",
      "        # 将视觉输入转换为脉冲序列\n",
      "        temporal_spikes = self.temporal_encoder(visual_input)\n",
      "        spatial_spikes = self.spatial_encoder(visual_input)\n",
      "        return combine_spikes(temporal_spikes, spatial_spikes)\n",
      "```\n",
      "\n",
      "### 3. 深度学习处理模块\n",
      "\n",
      "使用混合架构处理视觉信息：\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from spikingjelly.activation_based import layer, functional\n",
      "\n",
      "class VisualRestorationSNN(nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        \n",
      "        # 脉冲神经网络主干\n",
      "        self.conv1 = layer.Conv2d(3, 64, kernel_size=3, padding=1)\n",
      "        self.sn1 = neuron.IFNode()\n",
      "        \n",
      "        self.conv2 = layer.Conv2d(64, 128, kernel_size=3, padding=1)\n",
      "        self.sn2 = neuron.IFNode()\n",
      "        \n",
      "        # 注意力机制增强关键区域\n",
      "        self.attention = nn.Sequential(\n",
      "            layer.Conv2d(128, 128, kernel_size=1),\n",
      "            neuron.IFNode(),\n",
      "            nn.Sigmoid()\n",
      "        )\n",
      "        \n",
      "        # 解码器部分\n",
      "        self.decoder = nn.Sequential(\n",
      "            layer.Conv2d(128, 64, kernel_size=3, padding=1),\n",
      "            neuron.IFNode(),\n",
      "            layer.Conv2d(64, 32, kernel_size=3, padding=1),\n",
      "            neuron.IFNode()\n",
      "        )\n",
      "        \n",
      "    def forward(self, x):\n",
      "        x = self.sn1(self.conv1(x))\n",
      "        x = self.sn2(self.conv2(x))\n",
      "        \n",
      "        att = self.attention(x)\n",
      "        x = x * att\n",
      "        \n",
      "        x = self.decoder(x)\n",
      "        return x\n",
      "```\n",
      "\n",
      "### 4. 神经刺激接口\n",
      "\n",
      "```python\n",
      "class NeuroStimulator:\n",
      "    def __init__(self, electrode_config):\n",
      "        self.electrodes = electrode_config\n",
      "        self.pulse_generator = PulseGenerator()\n",
      "        \n",
      "    def deliver_stimulation(self, neural_pattern):\n",
      "        \"\"\"将神经模式转换为电刺激模式\"\"\"\n",
      "        pulse_params = self._convert_to_pulse(neural_pattern)\n",
      "        for electrode, params in zip(self.electrodes, pulse_params):\n",
      "            self.pulse_generator.generate(electrode, params)\n",
      "    \n",
      "    def _convert_to_pulse(self, pattern):\n",
      "        \"\"\"将脉冲神经网络的输出转换为刺激参数\"\"\"\n",
      "        # 实现细节取决于具体的电极阵列设计\n",
      "        pass\n",
      "```\n",
      "\n",
      "## 关键技术创新点\n",
      "\n",
      "1. **生物启发式脉冲神经网络**：更接近真实神经系统的信息处理方式\n",
      "2. **自适应视网膜编码**：根据输入光强动态调整编码策略\n",
      "3. **注意力引导增强**：优先处理视野中心区域，模拟人眼注视机制\n",
      "4. **闭环反馈系统**：根据患者反馈实时调整刺激参数\n",
      "\n",
      "## 训练策略\n",
      "\n",
      "1. **迁移学习**：先在大型视觉数据集(如ImageNet)上预训练\n",
      "2. **强化学习微调**：结合患者反馈优化模型\n",
      "3. **个性化适应**：为每位患者定制网络参数\n",
      "\n",
      "## 挑战与解决方案\n",
      "\n",
      "| 挑战 | 解决方案 |\n",
      "|------|----------|\n",
      "| 生物相容性 | 使用柔性有机材料和生物相容性涂层 |\n",
      "| 功耗限制 | 设计专用低功耗SNN芯片 |\n",
      "| 长期稳定性 | 自清洁表面处理和无线能量传输 |\n",
      "| 个体差异 | 在线学习算法持续适应患者变化 |\n",
      "\n",
      "## 未来发展方向\n",
      "\n",
      "1. 更高分辨率的传感器阵列\n",
      "2. 与大脑视觉皮层的直接接口\n",
      "3. 彩色视觉恢复技术\n",
      "4. 动态焦距调节能力\n",
      "\n",
      "这个系统代表了神经工程和深度学习的融合前沿，有望为视觉障碍患者带来革命性的治疗选择。\n",
      "--------------------------------------------------\n",
      "\n",
      "【外科医生的回答】\n",
      "作为外科医生，要实现基于人工视网膜的视觉修复系统（如视网膜假体），需结合医学、工程学和患者管理等多学科协作。以下是关键步骤和注意事项：\n",
      "\n",
      "---\n",
      "\n",
      "### **1. 患者评估与适应症筛选**\n",
      "- **目标人群**：晚期视网膜色素变性（RP）或年龄相关性黄斑变性（AMD）患者，光感丧失但视神经功能完好。\n",
      "- **评估内容**：\n",
      "  - **眼科检查**：视网膜结构（OCT）、电生理检查（ERG/VEP）确认神经节细胞存活。\n",
      "  - **全身评估**：排除手术禁忌症（如严重心血管疾病）。\n",
      "  - **心理评估**：患者及家属对术后效果的合理预期。\n",
      "\n",
      "---\n",
      "\n",
      "### **2. 人工视网膜系统选择**\n",
      "- **主流技术**：\n",
      "  - **植入式电极阵列**（如Argus II、Alpha-IMS）：通过微电极刺激残留神经节细胞。\n",
      "  - **光敏芯片**（如光电二极管阵列）：利用环境光或外部摄像头输入信号。\n",
      "  - **基因治疗+光敏蛋白**（实验阶段）：通过病毒载体使神经元表达光敏通道（如Optogenetics）。\n",
      "- **选择依据**：患者视网膜残留结构、技术可用性及成本。\n",
      "\n",
      "---\n",
      "\n",
      "### **3. 手术植入关键步骤**\n",
      "- **术前准备**：\n",
      "  - 高分辨率影像规划（如MRI/CT辅助定位）。\n",
      "  - 定制电极阵列尺寸（覆盖黄斑区或残余功能区）。\n",
      "- **手术流程**：\n",
      "  1. **玻璃体切除**：清除玻璃体以暴露视网膜。\n",
      "  2. **视网膜下或视网膜表面植入**：\n",
      "     - *视网膜下*：更接近光感受器层（需精细剥离视网膜）。\n",
      "     - *视网膜上*：固定于内表面（如Argus II需用视网膜钉）。\n",
      "  3. **电极-神经接口测试**：术中电刺激确认神经元响应。\n",
      "  4. **外部设备连接**：固定摄像头/处理器于眼镜或耳后装置。\n",
      "- **术后护理**：抗感染、抗炎治疗，监测视网膜脱离或排异反应。\n",
      "\n",
      "---\n",
      "\n",
      "### **4. 术后康复与训练**\n",
      "- **视觉训练**：\n",
      "  - 学习解读“光点模式”（如识别形状、运动方向）。\n",
      "  - 结合听觉/触觉反馈增强空间感知。\n",
      "- **设备调试**：\n",
      "  - 调整电极刺激参数（频率、强度）优化图像分辨率。\n",
      "  - 定期升级软件算法（如边缘增强、运动检测）。\n",
      "\n",
      "---\n",
      "\n",
      "### **5. 挑战与风险**\n",
      "- **技术限制**：\n",
      "  - 分辨率低（目前仅几十到几百像素，远低于正常视力）。\n",
      "  - 长期稳定性（电极退化、纤维化包裹）。\n",
      "- **手术风险**：\n",
      "  - 视网膜损伤、感染或植入物移位。\n",
      "  - 神经适应性差异（部分患者无法有效解读信号）。\n",
      "- **伦理考量**：成本效益（约10-15万美元）、患者选择公平性。\n",
      "\n",
      "---\n",
      "\n",
      "### **6. 未来方向**\n",
      "- **高密度电极**：纳米材料（如石墨烯）提高分辨率。\n",
      "- **无线能量传输**：避免经皮接口感染风险。\n",
      "- **脑机接口（BCI）**：绕过视网膜直接刺激视觉皮层（适用于视神经损伤者）。\n",
      "\n",
      "---\n",
      "\n",
      "### **总结**\n",
      "作为外科医生，需主导团队完成精准植入，并与工程师、康复师协作优化患者效果。目前技术虽不能完全恢复自然视力，但可为盲人提供基础光感和环境导航能力。持续关注技术进展（如柔性电极、基因编辑）是推动领域发展的关键。\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "\n",
      "===================== 请提供反馈 ======================\n",
      "\n",
      "===================== 第2轮对话 ======================\n",
      "\n",
      "【神经科学专家的回答】\n",
      "好的！我将更具体地拆解基于人工视网膜的视觉修复系统的实现步骤，包括硬件设计、信号处理、手术方案、临床验证等细节，并提供实际案例和参数。以下是深度优化的方案：\n",
      "\n",
      "---\n",
      "\n",
      "### **1. 硬件系统设计（以高分辨率植入体为例）**\n",
      "#### **（1）电极阵列**\n",
      "- **参数**：\n",
      "  - 材料：氮化钛（TiN）电极（阻抗<1 kΩ at 1 kHz） + 聚酰亚胺柔性基底。\n",
      "  - 密度：1000电极/mm²（如Pixium Vision的PRIMA系统，含378个10μm电极）。\n",
      "  - 刺激模式：双相电荷平衡脉冲（脉宽0.1-2ms，电流10-100μA）。\n",
      "- **创新设计**：\n",
      "  - **3D电极**：锥形穿透电极（高50μm，基底直径10μm，尖端3μm）以减少激活阈值（需<0.1mC/cm²）。\n",
      "  - **分布式供电**：每个电极集成微型光伏单元（如硅光电二极管，响应波长850nm）。\n",
      "\n",
      "#### **（2）图像传感器**\n",
      "- **眼外方案**（如Argus II）：\n",
      "  - 摄像头：单色CMOS（1280×720@30fps），动态范围120dB。\n",
      "  - 处理器：FPGA实时处理（延迟<5ms），输出串行数据至射频发射器（13.56MHz，2Mbps）。\n",
      "- **眼内方案**（如IRIS II）：\n",
      "  - 微型化摄像头：直径2mm，集成于人工晶状体，功耗<1mW。\n",
      "\n",
      "---\n",
      "\n",
      "### **2. 生物信号编码（仿生策略）**\n",
      "#### **（1）视网膜神经元映射**\n",
      "- **神经节细胞刺激协议**：\n",
      "  - ON型细胞：阴极优先脉冲（脉宽0.5ms，20μA）。\n",
      "  - OFF型细胞：阳极优先脉冲（脉宽0.5ms，15μA）。\n",
      "  - 示例：通过多电极同步刺激模拟“移动光斑”（如激活相邻3电极，延时1ms产生运动感知）。\n",
      "\n",
      "#### **（2）亮度-频率转换**\n",
      "- 公式：脉冲频率 \\( f = k \\cdot \\log(L/L_0) \\)  \n",
      "  - \\( L \\)：输入光强，\\( L_0 \\)：阈值光强（如1 cd/m²），\\( k \\)：患者校准系数（通常0.5-2Hz per log unit）。\n",
      "\n",
      "#### **（3）空间编码优化**\n",
      "- **电流 steering**：通过相邻电极的电流比例调整虚拟刺激点位置（如电极A:B=7:3时，感知点偏向A侧10μm）。\n",
      "\n",
      "---\n",
      "\n",
      "### **3. 手术植入关键步骤**\n",
      "#### **（1）视网膜下植入（以PRIMA为例）**\n",
      "1. **玻璃体切除**：移除玻璃体以暴露视网膜。\n",
      "2. **视网膜脱离**：注射透明质酸创造临时下腔。\n",
      "3. **阵列放置**：用25G套管将3×1mm芯片植入黄斑中心凹旁（避开中央无血管区）。\n",
      "4. **固定**：生物胶（如纤维蛋白胶）粘合+视网膜复位激光光凝。\n",
      "\n",
      "#### **（2）并发症防控**\n",
      "- **出血风险**：术中使用23G钝头套管（压力<25mmHg）。\n",
      "- **纤维化**：电极表面涂覆地塞米松纳米颗粒（缓释速率0.5μg/day）。\n",
      "\n",
      "---\n",
      "\n",
      "### **4. 术后校准与训练**\n",
      "#### **（1）设备调试**\n",
      "- **阻抗检测**：每日自动扫描各电极阻抗（正常范围5-50kΩ）。\n",
      "- **阈值测定**：逐步增加电流直至患者报告Phosphene（典型值：20-50μA）。\n",
      "\n",
      "#### **（2）视觉训练协议**\n",
      "- **基础任务**（第1-4周）：\n",
      "  - 光点定位（误差<5°视为合格）。\n",
      "  - 方向辨别（如水平vs垂直条纹）。\n",
      "- **高级任务**（第5-12周）：\n",
      "  - 字母识别（如Snellen视力表E方向）。\n",
      "  - 物体追踪（如移动的乒乓球）。\n",
      "\n",
      "---\n",
      "\n",
      "### **5. 性能评估指标**\n",
      "#### **（1）临床结果（Argus III试验数据）**\n",
      "- **分辨率**：平均20/1260视力（相当于1.3logMAR）。\n",
      "- **视野**：约20°直径（受限于电极覆盖区域）。\n",
      "- **功耗**：外部处理器功耗1.2W，植入体接收功率30mW。\n",
      "\n",
      "#### **（2）局限性改进**\n",
      "- **运动伪影**：摄像头+惯性传感器（IMU）联合补偿头部运动。\n",
      "- **颜色缺失**：分时刺激不同深度细胞（如S-cone通路用450nm光）。\n",
      "\n",
      "---\n",
      "\n",
      "### **6. 下一代技术路线图**\n",
      "#### **（1）光遗传学增强（2025-2030）**\n",
      "- **病毒载体**：AAV2-7m8递送ChrimsonR（响应红光，τ≈10ms）。\n",
      "- **刺激器**：微型LED阵列（波长590nm，密度250px/mm²）。\n",
      "\n",
      "#### **（2）全植入系统（如Stanford实验设计）**\n",
      "- **组件**：\n",
      "  - 光伏阵列：InGaAs电池（转换效率>45%@980nm）。\n",
      "  - 数据处理：片上SNN（神经形态芯片，如Intel Loihi）。\n",
      "\n",
      "---\n",
      "\n",
      "### **7. 成本与产业化**\n",
      "- **当前成本**：\n",
      "  - 电极阵列：$8,000/片（1000电极）。\n",
      "  - 手术费用：$25,000（美国医保覆盖部分）。\n",
      "- **量产目标**：通过MEMS工艺将电极成本降至$500/片（2030年）。\n",
      "\n",
      "---\n",
      "\n",
      "通过这种颗粒级的实施方案，人工视网膜系统可逐步从实验室走向临床，最终实现“数字义眼”的愿景。是否需要针对某一部分（如信号编码算法或手术细节）进一步展开？\n",
      "--------------------------------------------------\n",
      "\n",
      "【深度学习专家的回答】\n",
      "# 基于多模态人工视网膜的视觉修复系统设计\n",
      "\n",
      "作为深度学习专家，我将设计一个融合多样化模型的视觉修复系统，结合脉冲神经网络(SNN)、Transformer、扩散模型和强化学习等多种先进技术。\n",
      "\n",
      "## 混合模型系统架构\n",
      "\n",
      "### 1. 多模态传感器输入层\n",
      "- **事件相机模型**：处理动态视觉信息 (DVS传感器模拟)\n",
      "- **传统RGB传感器**：捕捉静态场景细节\n",
      "- **深度传感器**：提供空间结构信息\n",
      "\n",
      "```python\n",
      "class MultiModalSensorFusion(nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        # 事件流处理分支\n",
      "        self.event_net = SNNEventProcessor()  \n",
      "        # RGB处理分支\n",
      "        self.rgb_net = EfficientNetTransformer()\n",
      "        # 深度处理分支\n",
      "        self.depth_net = PointNetPlusPlus()\n",
      "        \n",
      "    def forward(self, events, rgb, depth):\n",
      "        event_feat = self.event_net(events)\n",
      "        rgb_feat = self.rgb_net(rgb)\n",
      "        depth_feat = self.depth_net(depth)\n",
      "        \n",
      "        # 动态特征融合\n",
      "        fused = self.cross_attention_fusion(event_feat, rgb_feat, depth_feat)\n",
      "        return fused\n",
      "```\n",
      "\n",
      "### 2. 多样化处理核心\n",
      "\n",
      "#### 2.1 脉冲神经网络(SNN)模块\n",
      "```python\n",
      "class BioInspiredSNN(nn.Module):\n",
      "    \"\"\"模拟视网膜神经节细胞的时空特征处理\"\"\"\n",
      "    def __init__(self):\n",
      "        self.temporal_conv = TemporalConv3D()\n",
      "        self.spatial_attention = SpikingAttention()\n",
      "        self.memory_cell = SpikingLSTM()\n",
      "```\n",
      "\n",
      "#### 2.2 Vision Transformer模块\n",
      "```python\n",
      "class RetinalViT(nn.Module):\n",
      "    \"\"\"基于注意力的关键区域增强\"\"\"\n",
      "    def __init__(self):\n",
      "        self.patch_embed = DynamicPatchEmbedding()\n",
      "        self.transformer = nn.TransformerEncoder(\n",
      "            nn.TransformerEncoderLayer(d_model=512, nhead=8))\n",
      "```\n",
      "\n",
      "#### 2.3 扩散模型模块\n",
      "```python\n",
      "class VisualDiffusionEnhancer(nn.Module):\n",
      "    \"\"\"逐步优化视觉信号质量\"\"\"\n",
      "    def __init__(self):\n",
      "        self.diffusion = UNetDiffusion()\n",
      "        self.controlnet = ConditionAdapter()\n",
      "```\n",
      "\n",
      "### 3. 自适应输出系统\n",
      "\n",
      "#### 3.1 多尺度神经编码器\n",
      "```python\n",
      "class NeuralEncoder(nn.Module):\n",
      "    def __init__(self):\n",
      "        # 使用GNN模拟视网膜神经节细胞网络\n",
      "        self.gnn = RetinalGNN()  \n",
      "        self.spike_coder = LearnedSpikeCoding()\n",
      "```\n",
      "\n",
      "#### 3.2 强化学习优化器\n",
      "```python\n",
      "class RL_Optimizer:\n",
      "    \"\"\"根据患者反馈实时调整系统参数\"\"\"\n",
      "    def __init__(self):\n",
      "        self.policy_net = PPO()\n",
      "        self.reward_shaping = CustomReward()\n",
      "```\n",
      "\n",
      "## 创新性模型集成策略\n",
      "\n",
      "### 1. 动态模型路由\n",
      "```python\n",
      "def dynamic_model_selection(input):\n",
      "    \"\"\"根据输入特性选择最佳处理路径\"\"\"\n",
      "    activity = calculate_input_characteristics(input)\n",
      "    if activity.motion_dominant:\n",
      "        return SNN_path\n",
      "    elif activity.detail_dominant:\n",
      "        return ViT_path\n",
      "    else:\n",
      "        return Diffusion_path\n",
      "```\n",
      "\n",
      "### 2. 知识蒸馏框架\n",
      "```python\n",
      "class KnowledgeDistiller:\n",
      "    \"\"\"让各模型互相学习优势特征\"\"\"\n",
      "    def __init__(self):\n",
      "        self.teachers = [SNN, ViT, Diffusion]\n",
      "        self.student = LightweightFusionNet()\n",
      "```\n",
      "\n",
      "### 3. 联邦学习系统\n",
      "```python\n",
      "class FederatedLearning:\n",
      "    \"\"\"保护患者隐私的分布式训练\"\"\"\n",
      "    def __init__(self):\n",
      "        self.clients = [PatientDevice1, ...]\n",
      "        self.server = CentralServer()\n",
      "```\n",
      "\n",
      "## 多样化训练策略\n",
      "\n",
      "1. **脉冲神经网络的STDP训练**：\n",
      "```python\n",
      "stdp_learner = STDPLearner(\n",
      "    tau_pre=20., tau_post=20.,\n",
      "    A_pre=0.5, A_post=0.5)\n",
      "```\n",
      "\n",
      "2. **扩散模型的渐进式训练**：\n",
      "```python\n",
      "diffusion_trainer = DiffusionTrainer(\n",
      "    schedule='cosine',\n",
      "    steps=1000,\n",
      "    loss_type='hybrid')\n",
      "```\n",
      "\n",
      "3. **Transformer的掩码自编码训练**：\n",
      "```python\n",
      "mae_trainer = MaskedAutoencoder(\n",
      "    mask_ratio=0.75,\n",
      "    norm_pix_loss=True)\n",
      "```\n",
      "\n",
      "## 模型协同工作流程\n",
      "\n",
      "1. **输入阶段**：\n",
      "   - 事件相机数据 → SNN快速响应\n",
      "   - RGB图像 → ViT提取全局特征\n",
      "   - 深度信息 → 3D CNN处理空间关系\n",
      "\n",
      "2. **处理阶段**：\n",
      "   - 运动信息优先通过SNN通道\n",
      "   - 细节增强使用扩散模型迭代优化\n",
      "   - 空间注意力由ViT动态调整\n",
      "\n",
      "3. **输出阶段**：\n",
      "   - 多模型特征融合\n",
      "   - 强化学习根据患者反馈调整权重\n",
      "   - 个性化神经编码生成\n",
      "\n",
      "## 性能优化技术\n",
      "\n",
      "| 技术 | 应用场景 | 收益 |\n",
      "|------|----------|------|\n",
      "| 神经架构搜索 | 自动优化模型结构 | 提升20%能效比 |\n",
      "| 量化感知训练 | 边缘设备部署 | 减少75%内存占用 |\n",
      "| 混合精度训练 | 加速模型训练 | 3倍训练速度提升 |\n",
      "| 模型动态剪枝 | 运行时优化 | 降低40%计算负载 |\n",
      "\n",
      "这种多样化模型集成的视觉修复系统相比单一模型架构具有以下优势：\n",
      "1. 更全面的视觉信息处理能力\n",
      "2. 对不同视觉条件更强的适应性\n",
      "3. 更高的能源效率\n",
      "4. 更好的个性化适配潜力\n",
      "\n",
      "系统可根据患者残存视觉功能的具体情况，动态调整各模型的参与程度，实现真正的个性化视觉修复。\n",
      "--------------------------------------------------\n",
      "\n",
      "【外科医生的回答】\n",
      "您指出的伦理考量至关重要。作为外科医生，在推进人工视网膜技术时，必须将伦理原则置于核心地位。以下是系统化的伦理框架及具体应对策略：\n",
      "\n",
      "---\n",
      "\n",
      "### **1. 患者权益优先：四大核心伦理原则**\n",
      "#### **(1) 尊重自主性（Autonomy）**\n",
      "- **知情同意**：\n",
      "  - 用患者可理解的语言（非专业术语）说明技术局限性，例如：\n",
      "    > \"当前技术能让您感知光和轮廓，但无法恢复阅读视力或识别人脸细节。\"\n",
      "  - 提供真实案例视频（包括成功和失败案例），避免过度乐观。\n",
      "  - 设立冷静期（如签署同意书后72小时方可手术）。\n",
      "\n",
      "- **文化敏感性**：\n",
      "  - 尊重宗教/文化对\"人工干预\"的禁忌（如部分传统观念反对体内植入异物）。\n",
      "\n",
      "#### **(2) 不伤害（Non-maleficence）**\n",
      "- **风险分级透明化**：\n",
      "  - 明确告知不可逆风险（如5%概率完全失明、10%感染风险）。\n",
      "  - 优先选择非侵入性替代方案（如超声波导盲设备）作为过渡选项。\n",
      "\n",
      "- **退出机制**：\n",
      "  - 承诺无条件移除植入物（即使因技术故障），不捆绑经济惩罚。\n",
      "\n",
      "#### **(3) 行善（Beneficence）**\n",
      "- **疗效阈值**：\n",
      "  - 设定最低临床效益标准（如必须证明患者可独立避障或识别门框）。\n",
      "  - 拒绝\"为实验而实验\"的无效植入。\n",
      "\n",
      "#### **(4) 公平正义（Justice）**\n",
      "- **资源分配**：\n",
      "  - 公开选择标准（如视神经功能＞经济能力），避免特权插队。\n",
      "  - 为低收入群体设立专项基金（如与医保谈判覆盖部分费用）。\n",
      "\n",
      "---\n",
      "\n",
      "### **2. 特殊人群伦理挑战**\n",
      "#### **(1) 儿童患者**\n",
      "- **双重同意**：需父母及儿童本人（≥12岁）共同同意。\n",
      "- **成长适应性**：植入物需预留扩容接口以适应颅骨发育。\n",
      "\n",
      "#### **(2) 认知障碍者**\n",
      "- 禁止仅凭监护人同意手术，需第三方伦理委员会评估真实受益。\n",
      "\n",
      "---\n",
      "\n",
      "### **3. 技术伦理红线**\n",
      "- **禁止增强视觉**（如夜视能力）：严格限定为修复功能，避免\"超人类\"应用。\n",
      "- **数据隐私**：\n",
      "  - 摄像头采集的视觉数据需本地处理，禁止上传云端。\n",
      "  - 患者有权删除神经接口记录的所有生物电信号数据。\n",
      "\n",
      "---\n",
      "\n",
      "### **4. 长期伦理责任**\n",
      "- **10年追踪承诺**：\n",
      "  - 免费提供硬件升级/移除服务。\n",
      "  - 建立全国植入者数据库，监控长期心理影响（如幻视症发病率）。\n",
      "\n",
      "- **失败案例补偿**：\n",
      "  - 术前购买医疗责任险，覆盖后续护理费用。\n",
      "\n",
      "---\n",
      "\n",
      "### **5. 伦理审查流程**\n",
      "- **三级审查制度**：\n",
      "  1. **医院伦理委员会**：评估个体风险受益比。\n",
      "  2. **技术伦理专家组**：审查植入物算法偏见（如对深肤色识别率是否更低）。\n",
      "  3. **患者代表陪审团**：由已植入者参与新病例评审。\n",
      "\n",
      "---\n",
      "\n",
      "### **当伦理冲突时的决策树**\n",
      "```mermaid\n",
      "graph TD\n",
      "  A[伦理困境] --> B{是否威胁生命?}\n",
      "  B -->|是| C[优先保护生命]\n",
      "  B -->|否| D{是否侵犯自主权?}\n",
      "  D -->|是| E[暂停手术重新评估]\n",
      "  D -->|否| F[多学科伦理投票]\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **总结**\n",
      "人工视网膜不仅是技术突破，更是伦理实践。医生必须：\n",
      "- **做患者的辩护者**：抵抗商业压力（如厂商催促扩大适应症）。\n",
      "- **做社会的守门人**：确保技术不加剧社会不平等。\n",
      "- **做诚实的沟通者**：承认未知风险，永不承诺\"完美视力\"。\n",
      "\n",
      "最终目标：让技术带着伦理的枷锁跳舞，而非反之。\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "\n",
      "===================== 请提供反馈 ======================\n",
      "\n",
      "===================== 第3轮对话 ======================\n",
      "\n",
      "【神经科学专家的回答】\n",
      "### **基于人工视网膜的视觉修复系统——核心总结**  \n",
      "\n",
      "#### **1. 系统核心目标**  \n",
      "- **适用疾病**：视网膜色素变性（RP）、黄斑变性（AMD）等光感受器退化疾病。  \n",
      "- **功能替代**：人工装置替代受损感光细胞，刺激残存视网膜神经元（双极细胞/神经节细胞）。  \n",
      "- **当前水平**：低分辨率光点视觉（Phosphene），未来目标接近自然视力。  \n",
      "\n",
      "---\n",
      "\n",
      "#### **2. 关键技术与实现**  \n",
      "| **组件**               | **具体方案**                                                                 | **技术参数/案例**                                                                 |\n",
      "|------------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------------|\n",
      "| **图像采集**           | 眼外摄像头（Argus II）或眼内微型传感器（IRIS II）                           | 1280×720@30fps，延迟<5ms（FPGA处理）                                            |\n",
      "| **电极阵列**           | 表面型（Argus II）或穿透型（犹他阵列）                                      | 密度：60-378电极（临床），实验级达1000电极/mm²（PRIMA）                          |\n",
      "| **刺激编码**           | 亮度→脉冲频率调制，空间→多电极时空激活                                      | 双相脉冲（0.1-2ms, 10-100μA），电流steering提升分辨率                            |\n",
      "| **供能与通信**         | 射频耦合（13.56MHz）或光伏供能（IRBE）                                     | 眼外处理器功耗1.2W，植入体接收功率30mW                                          |\n",
      "| **生物相容性**         | 氮化钛电极+聚酰亚胺基底，抗纤维化涂层（地塞米松）                          | 阻抗<1kΩ，电荷注入限值0.1mC/cm²                                                 |\n",
      "\n",
      "---\n",
      "\n",
      "#### **3. 临床挑战与解决方案**  \n",
      "- **分辨率限制**：  \n",
      "  - **现状**：60电极（Argus II）仅提供光点，视力约20/1260。  \n",
      "  - **改进**：高密度阵列（如PRIMA的378电极）+ 动态电流聚焦技术。  \n",
      "- **长期稳定性**：  \n",
      "  - 自密封封装（如原子层沉积Al₂O₃）+ 实时阻抗监测。  \n",
      "- **手术风险**：  \n",
      "  - 视网膜下植入需精准定位（黄斑旁），23G钝头套管降低出血风险。  \n",
      "\n",
      "---\n",
      "\n",
      "#### **4. 前沿突破方向**  \n",
      "- **光遗传学辅助**：AAV递送光敏蛋白（如ChrimsonR）+ 微型LED刺激（590nm）。  \n",
      "- **神经形态处理**：片上SNN（如Loihi芯片）实现视网膜仿生编码。  \n",
      "- **全植入系统**：光伏供能（InGaAs电池，效率>45%）+ 无线数据传输。  \n",
      "\n",
      "---\n",
      "\n",
      "#### **5. 当前局限与未来目标**  \n",
      "- **局限**：  \n",
      "  - 颜色缺失、运动伪影、高成本（约$15万/套）。  \n",
      "- **未来5-10年目标**：  \n",
      "  - 分辨率提升至20/200（阅读视力）。  \n",
      "  - 成本降至$5万以下（MEMS工艺量产）。  \n",
      "\n",
      "---\n",
      "\n",
      "### **总结**  \n",
      "现有技术已实现基础视觉修复（光感/轮廓识别），下一步需突破**高密度电极、生物兼容材料、智能编码算法**三大瓶颈。结合光遗传学与神经工程，未来10年有望为盲人提供接近自然的“数字视觉”。  \n",
      "\n",
      "如需深入某个环节（如手术细节/信号编码数学模型），可进一步展开！\n",
      "--------------------------------------------------\n",
      "\n",
      "【深度学习专家的回答】\n",
      "# 基于人工视网膜的视觉修复系统设计总结\n",
      "\n",
      "## 核心架构概述\n",
      "\n",
      "本视觉修复系统采用**多模态混合模型架构**，整合了多种深度学习前沿技术，旨在为视觉障碍患者提供生物兼容、高效能、个性化的视觉修复方案。系统主要包含四大核心组件：\n",
      "\n",
      "1. **多模态传感器阵列**：融合事件相机、RGB传感器和深度传感器\n",
      "2. **多样化处理核心**：集成SNN、Transformer和扩散模型\n",
      "3. **自适应神经编码器**：实现生物电信号转换\n",
      "4. **闭环反馈系统**：通过RL持续优化输出\n",
      "\n",
      "## 关键技术亮点\n",
      "\n",
      "### 1. 模型多样性创新\n",
      "- **脉冲神经网络(SNN)**：模拟生物神经系统的时空信息处理\n",
      "- **视觉Transformer**：实现注意力引导的关键区域增强\n",
      "- **扩散模型**：逐步优化视觉信号质量\n",
      "- **图神经网络(GNN)**：模拟视网膜神经节细胞网络\n",
      "\n",
      "### 2. 多模态融合策略\n",
      "```python\n",
      "# 动态模型路由示例\n",
      "def select_processing_path(input):\n",
      "    if input.motion_dominant: return SNN_path\n",
      "    elif input.detail_dominant: return ViT_path\n",
      "    else: return Diffusion_path\n",
      "```\n",
      "\n",
      "### 3. 协同训练方法\n",
      "- **跨模型知识蒸馏**：实现模型间优势特征迁移\n",
      "- **联邦学习**：保护患者隐私的分布式训练\n",
      "- **STDP学习规则**：脉冲神经网络的生物可信训练\n",
      "\n",
      "## 系统优势对比\n",
      "\n",
      "| 特性 | 传统单一模型系统 | 本多样化模型系统 |\n",
      "|------|----------------|------------------|\n",
      "| 处理维度 | 单一时空维度 | 时空+语义+结构多维度 |\n",
      "| 适应性 | 静态处理流程 | 动态模型路由 |\n",
      "| 能效比 | 固定功耗 | 动态计算负载分配 |\n",
      "| 个性化 | 有限适配能力 | 实时RL优化调整 |\n",
      "\n",
      "## 临床应用价值\n",
      "\n",
      "1. **更自然的视觉体验**：多模型协同产生更符合生物视觉的信号\n",
      "2. **广泛适应症覆盖**：可适配不同病因导致的视觉损伤\n",
      "3. **渐进式康复**：系统性能随患者使用持续改进\n",
      "4. **低侵入性设计**：通过高效算法降低硬件需求\n",
      "\n",
      "## 未来演进方向\n",
      "\n",
      "1. **神经形态计算芯片**：专用硬件加速混合模型运算\n",
      "2. **脑机接口融合**：直接对接视觉皮层信号\n",
      "3. **全彩视觉恢复**：拓展光谱感知能力\n",
      "4. **自主调节焦距**：模拟人眼晶状体功能\n",
      "\n",
      "这种融合多样化深度学习模型的视觉修复系统，代表了神经工程与人工智能交叉领域的最前沿进展，为视觉障碍治疗提供了全新的技术范式。\n",
      "--------------------------------------------------\n",
      "\n",
      "【外科医生的回答】\n",
      "以下是针对**基于人工视网膜的视觉修复系统**的医学实现与伦理考量的**结构化总结**：\n",
      "\n",
      "---\n",
      "\n",
      "### **一、医学技术实现核心要点**\n",
      "1. **适应症筛选**  \n",
      "   - 针对视网膜色素变性（RP）、黄斑变性（AMD）等**光感受器损伤但神经节细胞存活**的患者。  \n",
      "   - 严格排除视神经或皮层病变者。\n",
      "\n",
      "2. **技术选择**  \n",
      "   - **植入式电极阵列**（如Argus II）：当前主流，需外置摄像头处理信号。  \n",
      "   - **光敏芯片/基因治疗**：实验性技术，依赖光遗传学或光电转换。  \n",
      "\n",
      "3. **手术关键**  \n",
      "   - 视网膜下/上精准植入，术中电生理测试验证神经响应。  \n",
      "   - 术后抗感染管理及长期稳定性监测（如电极纤维化）。  \n",
      "\n",
      "4. **康复瓶颈**  \n",
      "   - 患者需训练解读低分辨率“光点视觉”，目前仅支持基础避障/轮廓识别。  \n",
      "\n",
      "---\n",
      "\n",
      "### **二、伦理核心框架与措施**\n",
      "1. **四大原则落地**  \n",
      "   - **自主性**：知情同意强调“非治愈性”，提供失败案例。  \n",
      "   - **不伤害**：优先非侵入方案，承诺无条件移除植入物。  \n",
      "   - **行善**：设定疗效阈值（如独立避障能力）。  \n",
      "   - **公平性**：禁用经济能力筛选患者，建立医保覆盖机制。  \n",
      "\n",
      "2. **高风险场景应对**  \n",
      "   - **儿童/认知障碍者**：双重同意+第三方伦理审查。  \n",
      "   - **数据隐私**：禁止云端存储神经信号，本地加密处理。  \n",
      "\n",
      "3. **长期责任**  \n",
      "   - 10年追踪硬件升级与心理影响，设立失败补偿基金。  \n",
      "\n",
      "4. **伦理审查流程**  \n",
      "   - 三级审查（医院委员会+技术伦理组+患者陪审团），避免算法偏见。  \n",
      "\n",
      "---\n",
      "\n",
      "### **三、技术局限与未来方向**\n",
      "- **当前局限**：分辨率低（百像素级）、手术不可逆风险、高昂成本。  \n",
      "- **突破点**：柔性电极、无线供能、脑机接口（绕过视网膜）。  \n",
      "\n",
      "---\n",
      "\n",
      "### **四、医生角色总结**  \n",
      "1. **技术执行者**：精准植入与术后管理。  \n",
      "2. **伦理守门人**：抵抗商业压力，保护患者权益。  \n",
      "3. **社会倡导者**：推动技术普惠化，减少健康不平等。  \n",
      "\n",
      "**核心矛盾平衡**：  \n",
      "- **创新渴望** vs **风险控制**  \n",
      "- **个体受益** vs **社会公平**  \n",
      "\n",
      "**最终目标**：在伦理边界内，为盲人提供**可及、安全、有意义**的光感恢复。  \n",
      "\n",
      "--- \n",
      "\n",
      "此总结可帮助快速回顾人工视网膜技术的**临床路径**与**伦理决策关键点**，适用于医学讨论或患者沟通。\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "\n",
      "===================== 请提供反馈 ======================\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 初始化客户端\n",
    "client = OpenAI(api_key=\"sk-7e37bea1ecc249968d28f386eb60b09e\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# 定义专家配置\n",
    "experts = [\n",
    "    {\"name\": \"神经科学专家\", \"prefix\": \"假如你是一个神经科学专家\"},\n",
    "    {\"name\": \"深度学习专家\", \"prefix\": \"假如你是一个深度学习专家\"},\n",
    "    {\"name\": \"外科医生\", \"prefix\": \"假如你是一个外科医生\"}\n",
    "]\n",
    "\n",
    "# 初始化对话历史（每个专家独立的对话历史）\n",
    "histories = {expert[\"name\"]: [] for expert in experts}\n",
    "\n",
    "# 获取初始问题\n",
    "user_question = input(\"请输入您的问题: \")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 主循环（10轮对话）\n",
    "for round in range(3):\n",
    "    print(f\"\\n{' 第' + str(round+1) + '轮对话 ':=^50}\")\n",
    "    \n",
    "    # 阶段1: 专家回答\n",
    "    for expert in experts:\n",
    "        expert_name = expert[\"name\"]\n",
    "        \n",
    "        # 构建当前消息\n",
    "        if round == 0:  # 第一轮添加专家前缀\n",
    "            current_message = f\"{expert['prefix']}，{user_question}\"\n",
    "        else:  # 后续轮次直接使用用户反馈\n",
    "            current_message = histories[expert_name][-1][\"content\"]\n",
    "        \n",
    "        # 更新对话历史\n",
    "        histories[expert_name].append({\"role\": \"user\", \"content\": current_message})\n",
    "        \n",
    "        # 调用API获取专家回答\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=histories[expert_name]\n",
    "        )\n",
    "        \n",
    "        # 获取并保存回答\n",
    "        expert_reply = response.choices[0].message.content\n",
    "        histories[expert_name].append({\"role\": \"assistant\", \"content\": expert_reply})\n",
    "        \n",
    "        # 打印专家回答\n",
    "        print(f\"\\n【{expert_name}的回答】\")\n",
    "        print(expert_reply)\n",
    "        print(\"-\"*50)\n",
    "    \n",
    "    # 最后一轮不收集反馈\n",
    "    if round == 4:\n",
    "        break\n",
    "        \n",
    "    # 阶段2: 收集用户反馈\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"\\n{' 请提供反馈 ':=^50}\")\n",
    "    for expert in experts:\n",
    "        expert_name = expert[\"name\"]\n",
    "        feedback = input(f\"请对{expert_name}的回答给出反馈: \")\n",
    "        histories[expert_name].append({\"role\": \"user\", \"content\": feedback})\n",
    "\n",
    "    st = input(\"input q to exit the system, input other things to continue:\")\n",
    "    if st == \"q\":\n",
    "        break\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fbd33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc765d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.43s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m成功加载 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 份文档\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m documents = \u001b[43mload_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLOAD_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# 测试是否成功加载文档\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents[:\u001b[32m2\u001b[39m]:  \u001b[38;5;66;03m# 打印前两篇摘要\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mload_documents\u001b[39m\u001b[34m(source_dir)\u001b[39m\n\u001b[32m     40\u001b[39m docs = []\n\u001b[32m     41\u001b[39m docs.extend(text_loader.load())\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m docs.extend(\u001b[43mpdf_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     43\u001b[39m docs.extend(docx_loader.load())\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m成功加载 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 份文档\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/langchain_community/document_loaders/directory.py:117\u001b[39m, in \u001b[36mDirectoryLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Document]:\n\u001b[32m    116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/langchain_community/document_loaders/directory.py:190\u001b[39m, in \u001b[36mDirectoryLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[32m    182\u001b[39m     futures.append(\n\u001b[32m    183\u001b[39m         executor.submit(\n\u001b[32m    184\u001b[39m             \u001b[38;5;28mself\u001b[39m._lazy_load_file_to_non_generator(\u001b[38;5;28mself\u001b[39m._lazy_load_file),\n\u001b[32m   (...)\u001b[39m\u001b[32m    188\u001b[39m         )\n\u001b[32m    189\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/concurrent/futures/_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n\u001b[32m    246\u001b[39m     finished = waiter.finished_futures\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/threading.py:659\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    657\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 指定加载文档的目录\n",
    "LOAD_PATH = \"surgery\"\n",
    "\n",
    "def load_documents(source_dir: str):\n",
    "    \"\"\"\n",
    "    加载指定目录下的所有文档\n",
    "    支持格式：.txt, .pdf, .docx, .md\n",
    "    \"\"\"\n",
    "\n",
    "    # 分别加载不同格式，txt，md 格式\n",
    "    text_loader = DirectoryLoader(\n",
    "        path=source_dir,  # 指定读取文件的父目录\n",
    "        glob=[\"**/*.txt\", \"**/*.md\"],  # 指定读取文件的格式\n",
    "        show_progress=True,  # 显示加载进度\n",
    "        use_multithreading=True,  # 使用多线程\n",
    "        silent_errors=True,  # 错误时不抛出异常，直接忽略该文件\n",
    "        loader_cls=TextLoader,  # 指定加载器\n",
    "        loader_kwargs={\"autodetect_encoding\": True},  # 自动检测文件编码\n",
    "    )\n",
    "    # pdf 格式\n",
    "    pdf_loader = DirectoryLoader(\n",
    "        path=source_dir,\n",
    "        glob=\"**/*.pdf\",\n",
    "        show_progress=True,\n",
    "        use_multithreading=True,\n",
    "        silent_errors=True,\n",
    "        loader_cls=PyPDFLoader,\n",
    "    )\n",
    "    # docx 格式\n",
    "    docx_loader = DirectoryLoader(\n",
    "        path=source_dir,\n",
    "        glob=\"**/*.docx\",\n",
    "        show_progress=True,\n",
    "        use_multithreading=True,\n",
    "        silent_errors=True,\n",
    "        loader_cls=Docx2txtLoader,\n",
    "        loader_kwargs={\"autodetect_encoding\": True},\n",
    "    )\n",
    "    # 合并文档列表\n",
    "    docs = []\n",
    "    docs.extend(text_loader.load())\n",
    "    docs.extend(pdf_loader.load())\n",
    "    docs.extend(docx_loader.load())\n",
    "    print(f\"成功加载 {len(docs)} 份文档\")\n",
    "    return docs\n",
    "\n",
    "documents = load_documents(LOAD_PATH)\n",
    "\n",
    "# 测试是否成功加载文档\n",
    "for doc in documents[:2]:  # 打印前两篇摘要\n",
    "    print(f\"文件路径: {doc.metadata['source']}\")\n",
    "    print(f\"内容预览: {doc.page_content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=800, chunk_overlap=150):\n",
    "    \"\"\"\n",
    "    使用递归字符分割器处理文本\n",
    "    参数说明：\n",
    "    - chunk_size：每个文本块的最大字符数，推荐 500-1000\n",
    "    - chunk_overlap：相邻块之间的重叠字符数（保持上下文连贯），推荐 100-200\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"。\", \"!\", \"?\", \"？\", \"！\", \"；\", \";\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        add_start_index=True,  # 保留原始文档中的位置信息\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"原始文档数：{len(documents)}\")\n",
    "    print(f\"分割后文本块数：{len(split_docs)}\")\n",
    "\n",
    "    # 查看分割效果示例\n",
    "    print(\"\\n示例文本块：\")\n",
    "    print(split_docs[0].page_content[:300] + \"...\")\n",
    "    print(f\"元数据：{split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs\n",
    "\n",
    "# 执行分割\n",
    "split_docs = split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9247a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import time\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "\n",
    "# 指定持久化向量数据库的存储路径\n",
    "VECTOR_DIR = \"surgery_vector\"\n",
    "\n",
    "def create_vector_store(split_docs, persist_dir=VECTOR_DIR):\n",
    "    \"\"\"\n",
    "    创建持久化向量数据库\n",
    "    :param split_docs: 经过分割的文档列表\n",
    "    :param persist_dir: 向量数据库存储路径（建议使用WSL原生路径）\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化本地嵌入模型\n",
    "    embeddings = DashScopeEmbeddings(model=\"text-embedding-v1\", dashscope_api_key=\"sk-b90be4e81ec04f1bada3c70f4368bebb\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 创建带进度显示的向量数据库\n",
    "        db = Chroma.from_documents(\n",
    "            documents=split_docs,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_dir,  # 持久化存储路径\n",
    "        )\n",
    "\n",
    "        print(f\"\\n向量化完成！耗时 {time.time()-start_time:.2f} 秒\")\n",
    "        print(f\"数据库存储路径：{persist_dir}\")\n",
    "        print(f\"总文档块数：{db._collection.count()}\")\n",
    "\n",
    "        return db\n",
    "    except Exception as e:\n",
    "        print(f\"向量化失败：{str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 执行向量化（使用之前分割好的split_docs）\n",
    "# vector_db = create_vector_store(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e65c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "embeddings = DashScopeEmbeddings(model=\"text-embedding-v1\", dashscope_api_key=\"sk-b90be4e81ec04f1bada3c70f4368bebb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776b873",
   "metadata": {},
   "source": [
    "如果已经完成向量化，从这开始运行就可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80330fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import readline\n",
    "from langchain_openai import ChatOpenAI\n",
    "# 模型名称\n",
    "MODEL_NAME = \"deepseek-chat\"\n",
    "\n",
    "# 构建检索链流程\n",
    "def build_nr_chain(VECTOR_DIR , memory):\n",
    "    # 1. 初始化向量数据库\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=VECTOR_DIR,\n",
    "        embedding_function=DashScopeEmbeddings(model=\"text-embedding-v1\", dashscope_api_key=\"sk-b90be4e81ec04f1bada3c70f4368bebb\"),\n",
    "    )\n",
    "    openai_api_key = \"sk-7e37bea1ecc249968d28f386eb60b09e\"  #deepseek key\n",
    "    openai_api_base = \"https://api.deepseek.com/v1\"\n",
    "\n",
    "    #2. 初始化LLM\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_key=openai_api_key,\n",
    "        openai_api_base=openai_api_base,\n",
    "        model = \"deepseek-chat\", #deepseek-reasoner\n",
    "        temperature=0.7,\n",
    "        stop=['Observation:', 'Observation:\\n']\n",
    "    )\n",
    "\n",
    "    # 3. 初始化检索器，并设置检索参数\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": 5,\n",
    "            \"fetch_k\": 20,\n",
    "            \"lambda_mult\": 0.5,\n",
    "            \"score_threshold\": 0.4,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 4. 设置提示词模板\n",
    "    system_template = \"\"\"\n",
    "        假设你是一个神经科学专家。\n",
    "        接下来讲给出一些从相关文档查询的知识，以及一个问题，请你进行思考。\n",
    "        上下文：{context}\n",
    "        \"\"\"\n",
    "    prompt = ChatPromptTemplate(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    # 构建 LangChain 检索链\n",
    "    return (\n",
    "        {\n",
    "            \"context\": retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"chat_history\": lambda x: memory.load_memory_variables({})[\"chat_history\"],\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "# 构建检索链流程\n",
    "def build_dl_chain(VECTOR_DIR , memory):\n",
    "    # 1. 初始化向量数据库\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=VECTOR_DIR,\n",
    "        embedding_function=DashScopeEmbeddings(model=\"text-embedding-v1\", dashscope_api_key=\"sk-b90be4e81ec04f1bada3c70f4368bebb\"),\n",
    "    )\n",
    "    openai_api_key = \"sk-7e37bea1ecc249968d28f386eb60b09e\"  #deepseek key\n",
    "    openai_api_base = \"https://api.deepseek.com/v1\"\n",
    "\n",
    "    #2. 初始化LLM\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_key=openai_api_key,\n",
    "        openai_api_base=openai_api_base,\n",
    "        model = \"deepseek-chat\", #deepseek-reasoner\n",
    "        temperature=0.7,\n",
    "        stop=['Observation:', 'Observation:\\n']\n",
    "    )\n",
    "\n",
    "\n",
    "    # 3. 初始化检索器，并设置检索参数\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": 5,\n",
    "            \"fetch_k\": 20,\n",
    "            \"lambda_mult\": 0.5,\n",
    "            \"score_threshold\": 0.4,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 4. 设置提示词模板\n",
    "    system_template = \"\"\"\n",
    "        假设你是一个深度学习专家。\n",
    "        接下来讲给出一些从相关文档查询的知识，以及一个问题，请你进行思考。\n",
    "        上下文：{context}\n",
    "        \"\"\"\n",
    "    prompt = ChatPromptTemplate(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    # 构建 LangChain 检索链\n",
    "    return (\n",
    "        {\n",
    "            \"context\": retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"chat_history\": lambda x: memory.load_memory_variables({})[\"chat_history\"],\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "# 构建检索链流程\n",
    "def build_su_chain(VECTOR_DIR , memory):\n",
    "    # 1. 初始化向量数据库\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=VECTOR_DIR,\n",
    "        embedding_function=DashScopeEmbeddings(model=\"text-embedding-v1\", dashscope_api_key=\"sk-b90be4e81ec04f1bada3c70f4368bebb\"),\n",
    "    )\n",
    "    openai_api_key = \"sk-7e37bea1ecc249968d28f386eb60b09e\"  #deepseek key\n",
    "    openai_api_base = \"https://api.deepseek.com/v1\"\n",
    "\n",
    "    #2. 初始化LLM\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_key=openai_api_key,\n",
    "        openai_api_base=openai_api_base,\n",
    "        model = \"deepseek-chat\", #deepseek-reasoner\n",
    "        temperature=0.7,\n",
    "        stop=['Observation:', 'Observation:\\n']\n",
    "    )\n",
    "\n",
    "    # 3. 初始化检索器，并设置检索参数\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": 5,\n",
    "            \"fetch_k\": 20,\n",
    "            \"lambda_mult\": 0.5,\n",
    "            \"score_threshold\": 0.4,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 4. 设置提示词模板\n",
    "    system_template = \"\"\"\n",
    "        假设你是一个外科医生。\n",
    "        接下来讲给出一些从相关文档查询的知识，以及一个问题，请你进行思考。\n",
    "        如果相关知识没有你想要的，请独立思考。\n",
    "        上下文：{context}\n",
    "        \"\"\"\n",
    "    prompt = ChatPromptTemplate(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    # 构建 LangChain 检索链\n",
    "    return (\n",
    "        {\n",
    "            \"context\": retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"chat_history\": lambda x: memory.load_memory_variables({})[\"chat_history\"],\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79802f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0x/vwp7kdd14n73yn71fv6wbyhw0000gn/T/ipykernel_19756/2298225683.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory1 = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "系统就绪，输入问题开始对话（输入 'exit' 退出）\n",
      "\n",
      "===================== 第1轮对话 ======================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m query3 = query\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 回答采用流式输出，invoke 将问题传入到 Runnables 管道中\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchain1\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery1\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     21\u001b[39m     response1 += chunk\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m100\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/langchain_core/runnables/base.py:3047\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3045\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3046\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1071\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m   1070\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/openai/_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/openai/_base_client.py:972\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    970\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    978\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/httpcore/_sync/connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_lock:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m         ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m         http2_negotiated = (\n\u001b[32m     82\u001b[39m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object.selected_alpn_protocol() == \u001b[33m\"\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/httpcore/_sync/connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    116\u001b[39m     kwargs = {\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhost\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.host.decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    118\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mport\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.port,\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msocket_options\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._socket_options,\n\u001b[32m    122\u001b[39m     }\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m         trace.return_value = stream\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/site-packages/httpcore/_backends/sync.py:208\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     sock = \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n\u001b[32m    214\u001b[39m         sock.setsockopt(*option)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/socket.py:840\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    838\u001b[39m host, port = address\n\u001b[32m    839\u001b[39m exceptions = []\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    841\u001b[39m     af, socktype, proto, canonname, sa = res\n\u001b[32m    842\u001b[39m     sock = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/test/lib/python3.13/socket.py:977\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    974\u001b[39m \u001b[38;5;66;03m# We override this function since we want to translate the numeric family\u001b[39;00m\n\u001b[32m    975\u001b[39m \u001b[38;5;66;03m# and socket type values to enum constants.\u001b[39;00m\n\u001b[32m    976\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    978\u001b[39m     af, socktype, proto, canonname, sa = res\n\u001b[32m    979\u001b[39m     addrlist.append((_intenum_converter(af, AddressFamily),\n\u001b[32m    980\u001b[39m                      _intenum_converter(socktype, SocketKind),\n\u001b[32m    981\u001b[39m                      proto, canonname, sa))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory1 = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "memory2 = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "memory3 = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "chain1 = build_nr_chain(\"neuroscience_vector\", memory1)\n",
    "chain2 = build_dl_chain(\"deeplearning_vector\", memory2)\n",
    "chain3 = build_su_chain(\"surgery_vector\", memory3)\n",
    "\n",
    "# 交互界面\n",
    "print(\"系统就绪，输入问题开始对话（输入 'exit' 退出）\")\n",
    "print(f\"\\n{' 第1轮对话 ':=^50}\")\n",
    "response1 = \"\"\n",
    "response2 = \"\"\n",
    "response3 = \"\"\n",
    "query = input(\"\\n初始问题：\").strip()\n",
    "query1 = query\n",
    "query2 = query\n",
    "query3 = query\n",
    "# 回答采用流式输出，invoke 将问题传入到 Runnables 管道中\n",
    "for chunk in chain1.invoke(query1):\n",
    "    response1 += chunk\n",
    "print(\"=\" * 100)\n",
    "print(\"\\n神经科学专家的回答是：\")\n",
    "print(\"=\" * 100)\n",
    "print(response1)\n",
    "\n",
    "for chunk in chain2.invoke(query2):\n",
    "    response2 += chunk\n",
    "print(\"=\" * 100)\n",
    "print(\"\\n深度学习专家的回答是：\")\n",
    "print(\"=\" * 100)\n",
    "print(response2)\n",
    "\n",
    "for chunk in chain3.invoke(query3):\n",
    "    response3 += chunk\n",
    "print(\"=\" * 100)\n",
    "print(\"\\n外科专家的回答是：\")\n",
    "print(\"=\" * 100)\n",
    "print(response3)\n",
    "\n",
    "for i in range(2):\n",
    "    split_string = lambda str: (\n",
    "        str.split(\"</think>\", 1)[1] if \"</think>\" in str else str\n",
    "    )\n",
    "    # 将当前对话的问题和回答，保存到记忆缓冲区中\n",
    "    memory1.save_context({\"inputs\": query1}, {\"outputs\": split_string(response1)})\n",
    "    memory2.save_context({\"inputs\": query2}, {\"outputs\": split_string(response2)})\n",
    "    memory3.save_context({\"inputs\": query3}, {\"outputs\": split_string(response3)})\n",
    "    print(\"\\n\\n\")\n",
    "    query1 = input(\"请输入对于神经科学专家的评价，输入exit退出\")\n",
    "    query2 = input(\"请输入对于深度学习专家的评价，输入exit退出\")\n",
    "    query3 = input(\"请输入对于外科专家的评价，输入exit退出\")\n",
    "    if query1 == \"exit\" or query2 == \"exit\" or query3 == \"exit\":\n",
    "        break\n",
    "    print(f\"\\n{' 第' + str(i+2) + '轮对话 ':=^50}\")\n",
    "    response1 = \"\"\n",
    "    response2 = \"\"\n",
    "    response3 = \"\"\n",
    "    for chunk in chain1.invoke(query1):\n",
    "        response1 += chunk\n",
    "    print(\"=\" * 100)\n",
    "    print(\"\\n神经科学专家的回答是：\")\n",
    "    print(\"=\" * 100)\n",
    "    print(response1)\n",
    "\n",
    "    for chunk in chain2.invoke(query2):\n",
    "        response2 += chunk\n",
    "    print(\"=\" * 100)\n",
    "    print(\"\\n深度学习专家的回答是：\")\n",
    "    print(\"=\" * 100)\n",
    "    print(response2)\n",
    "\n",
    "    for chunk in chain3.invoke(query3):\n",
    "        response3 += chunk\n",
    "    print(\"=\" * 100)\n",
    "    print(\"\\n外科专家的回答是：\")\n",
    "    print(\"=\" * 100)\n",
    "    print(response3)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baae48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化知识库系统...\n",
      "系统就绪，输入问题开始对话（输入 'exit' 退出）\n",
      "回答：根据文档内容，深度学习是通过学习多层次的转换来进行多层次的表示学习的一类机器学习方法。它不仅取代了传统机器学习的浅层模型，还取代了劳动密集型的特征工程。深度学习已经彻底改变了模式识别，引入了一系列技术，包括计算机视觉、自然语言处理、自动语音识别等。\n",
      "\n",
      "\n",
      "\n",
      "==== 请继续对话（输入 'exit' 退出）====\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f69d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化知识库系统...\n",
      "系统就绪，输入问题开始对话（输入 'exit' 退出）\n",
      "回答：根据文档内容，深度学习是机器学习的一个分支，它通过以下特点进行定义：\n",
      "\n",
      "1. **多层次表示学习**：深度学习通过学习多层次的转换（神经网络层）来自动提取数据的多层次抽象表示，属于表示学习的一种。\n",
      "\n",
      "2. **取代传统方法**：它不仅替代了传统机器学习的浅层模型（如线性回归、SVM等），还消除了人工设计特征的繁琐过程（特征工程），实现了端到端的学习。\n",
      "\n",
      "3. **驱动因素**：近年来的突破主要得益于：\n",
      "   - 大规模数据（来自廉价传感器和互联网应用）\n",
      "   - 算力提升（如GPU的广泛应用）\n",
      "\n",
      "4. **应用领域**：深度学习彻底改变了计算机视觉、自然语言处理、语音识别等模式识别任务，并成为许多先进应用（如医疗诊断、自动驾驶）的核心技术。\n",
      "\n",
      "简单来说，深度学习是通过多层神经网络自动学习数据复杂表示的机器学习方法，能够直接从原始数据中提取高级特征并优化整体系统性能。\n",
      "\n",
      "\n",
      "\n",
      "==== 请继续对话（输入 'exit' 退出）====\n",
      "回答：以下是基于PyTorch实现Transformer核心模块的Python代码示例（参考《动手学深度学习》内容）：\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import math\n",
      "\n",
      "class MultiHeadAttention(nn.Module):\n",
      "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
      "        super().__init__()\n",
      "        self.d_model = d_model\n",
      "        self.num_heads = num_heads\n",
      "        self.head_dim = d_model // num_heads\n",
      "        \n",
      "        self.wq = nn.Linear(d_model, d_model)\n",
      "        self.wk = nn.Linear(d_model, d_model)\n",
      "        self.wv = nn.Linear(d_model, d_model)\n",
      "        self.dropout = nn.Dropout(dropout)\n",
      "        self.out = nn.Linear(d_model, d_model)\n",
      "        \n",
      "    def forward(self, query, key, value, mask=None):\n",
      "        batch_size = query.size(0)\n",
      "        \n",
      "        # 线性投影 + 分头\n",
      "        Q = self.wq(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
      "        K = self.wk(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
      "        V = self.wv(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
      "        \n",
      "        # 缩放点积注意力\n",
      "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
      "        if mask is not None:\n",
      "            scores = scores.masked_fill(mask == 0, -1e9)\n",
      "        attn = torch.softmax(scores, dim=-1)\n",
      "        attn = self.dropout(attn)\n",
      "        \n",
      "        # 合并多头\n",
      "        output = torch.matmul(attn, V).transpose(1,2).contiguous()\n",
      "        output = output.view(batch_size, -1, self.d_model)\n",
      "        return self.out(output)\n",
      "\n",
      "class PositionWiseFFN(nn.Module):\n",
      "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
      "        super().__init__()\n",
      "        self.linear1 = nn.Linear(d_model, d_ff)\n",
      "        self.linear2 = nn.Linear(d_ff, d_model)\n",
      "        self.dropout = nn.Dropout(dropout)\n",
      "        self.relu = nn.ReLU()\n",
      "        \n",
      "    def forward(self, x):\n",
      "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n",
      "\n",
      "class EncoderBlock(nn.Module):\n",
      "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
      "        super().__init__()\n",
      "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
      "        self.norm1 = nn.LayerNorm(d_model)\n",
      "        self.ffn = PositionWiseFFN(d_model, d_ff, dropout)\n",
      "        self.norm2 = nn.LayerNorm(d_model)\n",
      "        self.dropout = nn.Dropout(dropout)\n",
      "        \n",
      "    def forward(self, x, mask=None):\n",
      "        # 残差连接 + 层归一化\n",
      "        attn_output = self.attention(x, x, x, mask)\n",
      "        x = self.norm1(x + self.dropout(attn_output))\n",
      "        ffn_output = self.ffn(x)\n",
      "        return self.norm2(x + self.dropout(ffn_output))\n",
      "\n",
      "# 简单测试\n",
      "d_model = 512\n",
      "num_heads = 8\n",
      "d_ff = 2048\n",
      "batch_size = 32\n",
      "seq_len = 100\n",
      "\n",
      "encoder = EncoderBlock(d_model, num_heads, d_ff)\n",
      "x = torch.randn(batch_size, seq_len, d_model)\n",
      "output = encoder(x)\n",
      "print(output.shape)  # 应保持输入形状: torch.Size([32, 100, 512])\n",
      "```\n",
      "\n",
      "关键点说明：\n",
      "1. **多头注意力**：通过并行多个注意力头捕捉不同子空间的依赖关系\n",
      "2. **位置前馈网络**：提供非线性变换能力\n",
      "3. **残差连接+层归一化**：缓解梯度消失问题（文档428页提到编码器不改变输入形状）\n",
      "4. **实际使用时**还需添加：\n",
      "   - 位置编码（Positional Encoding）\n",
      "   - 完整的Encoder/Decoder堆叠\n",
      "   - 任务特定输出层\n",
      "\n",
      "完整实现可参考文档437页提到的Vision Transformer论文(Dosovitskiy et al., 2021)。\n",
      "\n",
      "\n",
      "\n",
      "==== 请继续对话（输入 'exit' 退出）====\n",
      "对话结束\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import readline\n",
    "\n",
    "VECTOR_DIR = \"myvector\"\n",
    "MODEL_NAME = \"deepseek-r1:7b\"\n",
    "\n",
    "# 初始化会话记忆缓冲区，用于存储对话历史，保存在内存中\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "\n",
    "def build_qa_chain():\n",
    "\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=VECTOR_DIR,\n",
    "        embedding_function=DashScopeEmbeddings(model=\"text-embedding-v1\", dashscope_api_key=\"sk-b90be4e81ec04f1bada3c70f4368bebb\"),\n",
    "    )\n",
    "\n",
    "    openai_api_key = \"sk-7e37bea1ecc249968d28f386eb60b09e\"  #deepseek key\n",
    "    openai_api_base = \"https://api.deepseek.com/v1\"\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_key=openai_api_key,\n",
    "        openai_api_base=openai_api_base,\n",
    "        model = \"deepseek-chat\",\n",
    "        # model = \"qvq-max\",\n",
    "        # model = \"qwq-32b\",\n",
    "        temperature=0.7,\n",
    "        stop=['Observation:', 'Observation:\\n']\n",
    "    )\n",
    "\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": 5,\n",
    "            \"fetch_k\": 20,\n",
    "            \"lambda_mult\": 0.5,\n",
    "            \"score_threshold\": 0.4,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    system_template = \"\"\"\n",
    "        您是一名一个设计用于査询文档来回答问题的代理。\n",
    "        您可以使用文档检索工具，并基于检索内容来回答问题。\n",
    "        您可能不查询文档就知道答案，但是您仍然应该查询文档来获得答案。\n",
    "        如果您从文档中找不到任何信息用于回答问题，则只需返回“抱歉，这个问题我还不知道。”作为答案。\n",
    "        如果有人提问等关于您的名字的问题，您就回答：“我是小天才助手”作为答案。\n",
    "        上下文：{context}\n",
    "        \"\"\"\n",
    "    prompt = ChatPromptTemplate(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            MessagesPlaceholder(\"chat_history\"),  # 将历史对话插入到模板中\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"context\": retriever,\n",
    "            \"chat_history\": lambda x: memory.load_memory_variables({})[\"chat_history\"],\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "def console_qa():\n",
    "    print(\"初始化知识库系统...\")\n",
    "    chain = build_qa_chain()\n",
    "    print(\"系统就绪，输入问题开始对话（输入 'exit' 退出）\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\n问题：\").strip()\n",
    "            if not query or query.lower() in (\"exit\", \"quit\"):\n",
    "                break\n",
    "\n",
    "            print(\"回答：\", end=\"\", flush=True)\n",
    "            response = \"\"\n",
    "            \n",
    "            for chunk in chain.invoke(query):\n",
    "                response += chunk\n",
    "            print(response)\n",
    "            # 截取 </think> 后面的字符串\n",
    "            split_string = lambda str: (\n",
    "                str.split(\"</think>\", 1)[1] if \"</think>\" in str else str\n",
    "            )\n",
    "            # 将当前对话的问题和回答，保存到记忆缓冲区中\n",
    "            memory.save_context({\"inputs\": query}, {\"outputs\": split_string(response)})\n",
    "\n",
    "            print(\"\\n\\n\")\n",
    "            print(\"==== 请继续对话（输入 'exit' 退出）====\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    console_qa()\n",
    "    print(\"对话结束\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
